{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53b1eb2a-e453-4367-962b-40a86fd4c64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total raw pairs: 41044\n"
     ]
    }
   ],
   "source": [
    "import os, json, random\n",
    "\n",
    "PROJECT_ROOT = \"/Users/jasleenkaur/Desktop/translit-consistency\"\n",
    "os.chdir(PROJECT_ROOT)\n",
    "\n",
    "with open(\"data/processed/aligned_pairs_high_conf.json\", encoding=\"utf-8\") as f:\n",
    "    raw_pairs = json.load(f)\n",
    "\n",
    "print(\"Total raw pairs:\", len(raw_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5b4195cb-15f0-469b-9559-adedec47254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(en.lower(), hi) for en, hi, _ in raw_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b36443ca-726f-4d45-8ab5-76fae4f82d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  32835\n",
      "Val:  4104\n",
      "Test:  4105\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(pairs)\n",
    "\n",
    "n = len(pairs)\n",
    "train = pairs[:int(0.8 * n)]\n",
    "val = pairs[int(0.8 * n):int(0.9 * n)]\n",
    "test = pairs[int(0.9 * n):]\n",
    "\n",
    "print(\"Train: \", len(train))\n",
    "print(\"Val: \", len(val))\n",
    "print(\"Test: \", len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8be4dd40-1e27-41a9-8523-bde3818a7425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English chars:  61\n",
      "Hindi chars:  80\n"
     ]
    }
   ],
   "source": [
    "def build_char_set(words):\n",
    "    chars = set()\n",
    "    for w in words:\n",
    "        chars.update(w)\n",
    "    return sorted(chars)\n",
    "\n",
    "en_words = [en for en, hi in pairs]\n",
    "hi_words = [hi for en, hi in pairs]\n",
    "\n",
    "en_chars = build_char_set(en_words)\n",
    "hi_chars = build_char_set(hi_words)\n",
    "\n",
    "print(\"English chars: \", len(en_chars))\n",
    "print(\"Hindi chars: \", len(hi_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10cf948f-3df0-4f80-9689-cc9d99faaa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = \"<pad>\"\n",
    "SOS = \"<s>\"\n",
    "EOS = \"</s>\"\n",
    "\n",
    "def build_vocab_with_tokens(chars):\n",
    "    vocab = [PAD, SOS, EOS] + chars\n",
    "    stoi = {c: i for i, c in enumerate(vocab)}\n",
    "    itos = {i: c for c, i in stoi.items()}\n",
    "    return vocab, stoi, itos\n",
    "\n",
    "en_vocab, en_stoi, en_itos = build_vocab_with_tokens(en_chars)\n",
    "hi_vocab, hi_stoi, hi_itos = build_vocab_with_tokens(hi_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8eb895fb-79ba-41af-98a4-4a9327c7427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_english(word):\n",
    "    return {\n",
    "        \"delhi\": \"dilli\",\n",
    "        \"bangalore\": \"bangalor\",\n",
    "        \"bengaluru\": \"bangalor\",\n",
    "        \"maharashta\": \"maharashtra\",\n",
    "    }.get(word.lower(), word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "81309475-2886-4da8-80cb-a8db0e64e060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(word, stoi):\n",
    "    if stoi is en_stoi:\n",
    "        word = normalize_english(word)\n",
    "    return [stoi[SOS]] + [stoi[c] for c in word] + [stoi[EOS]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9232e757-fc72-44da-8a4a-23e3bfcf8be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded train example:\n",
      "([1, 28, 29, 34, 24, 29, 2], [1, 52, 55, 38, 67, 36, 56, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "train_enc = [(encode(en, en_stoi), encode(hi, hi_stoi)) for en, hi in train]\n",
    "val_enc   = [(encode(en, en_stoi), encode(hi, hi_stoi)) for en, hi in val]\n",
    "test_enc  = [(encode(en, en_stoi), encode(hi, hi_stoi)) for en, hi in test]\n",
    "\n",
    "print(\"Encoded train example:\")\n",
    "print(train_enc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6d3bbf1-ba09-4738-9157-d27754bfd80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(seq, max_len, pad_id):\n",
    "    return seq + [pad_id] * (max_len - len(seq))\n",
    "\n",
    "max_en_len = max(len(x[0]) for x in train_enc)\n",
    "max_hi_len = max(len(x[1]) for x in train_enc)\n",
    "\n",
    "train_pad = [\n",
    "    (pad(src, max_en_len, en_stoi[PAD]),\n",
    "     pad(tgt, max_hi_len, hi_stoi[PAD]))\n",
    "    for src, tgt in train_enc\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9a86076-b402-4b50-bc9f-018418195d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6c52ca9d-b3a4-4709-8a20-c4af504eec73",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src = torch.tensor(\n",
    "    [src for src, tgt in train_pad],\n",
    "    dtype = torch.long\n",
    ").to(device)\n",
    "\n",
    "train_tgt = torch.tensor(\n",
    "    [tgt for src, tgt in train_pad],\n",
    "    dtype = torch.long\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2be21d12-61d5-475c-81aa-66e890f76162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx = 0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first = True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)\n",
    "        outputs, (h, c) = self.lstm(emb)\n",
    "        return outputs, h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "febec7f9-482d-4b32-8eb6-014888a77616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.scale = 1.0 / (hidden_dim ** 0.5)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        decoder_hidden: (B, H)\n",
    "        encoder_outputs: (B, src_len, H)\n",
    "        \"\"\"\n",
    "        # (B, src_len)\n",
    "        scores = torch.bmm(\n",
    "            encoder_outputs,\n",
    "            decoder_hidden.unsqueeze(2)\n",
    "        ).squeeze(2)\n",
    "\n",
    "        attn_weights = torch.softmax(scores * self.scale, dim=1)\n",
    "\n",
    "        # (B, H)\n",
    "        context = torch.bmm(\n",
    "            attn_weights.unsqueeze(1),\n",
    "            encoder_outputs\n",
    "        ).squeeze(1)\n",
    "\n",
    "        return context, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02ebacce-8ef2-4ef8-8cc7-a290e8f79469",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.attention = LuongAttention(hidden_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim + hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, h, c, encoder_outputs):\n",
    "        # x: (B, 1)\n",
    "        emb = self.embedding(x)  # (B, 1, E)\n",
    "\n",
    "        # Attention\n",
    "        context, _ = self.attention(h[-1], encoder_outputs)  # (B, H)\n",
    "        context = context.unsqueeze(1)  # (B, 1, H)\n",
    "\n",
    "        # LSTM\n",
    "        lstm_input = torch.cat([emb, context], dim=2)  # (B, 1, E+H)\n",
    "        output, (h, c) = self.lstm(lstm_input, (h, c))  # output: (B, 1, H)\n",
    "\n",
    "        logits = self.fc(output.squeeze(1))  # (B, vocab)\n",
    "        return logits, h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0735e780-67df-4148-b942-38e19d692217",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index = hi_stoi[PAD]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c3d9a592-b8cc-46a3-a3d5-983d548f14ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, pad_idx):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        src: (B, src_len)\n",
    "        tgt: (B, tgt_len)\n",
    "        \"\"\"\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, tgt_len, vocab_size).to(src.device)\n",
    "\n",
    "        encoder_outputs, h, c = model.encoder(src)\n",
    "\n",
    "        input_tok = tgt[:, 0].unsqueeze(1)\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            logits, h, c = self.decoder(input_tok, h, c, encoder_outputs)\n",
    "            outputs[:, t] = logits\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = logits.argmax(1).unsqueeze(1)  \n",
    "\n",
    "            input_tok = tgt[:, t].unsqueeze(1) if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "46160d39-82d7-4502-a41d-d318a7b21ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "\n",
    "encoder = Encoder(len(en_vocab), EMBED_DIM, HIDDEN_DIM).to(device)\n",
    "decoder = Decoder(len(hi_vocab), EMBED_DIM, HIDDEN_DIM).to(device)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, hi_stoi[PAD]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a9ec3a4d-8f83-4589-804c-62434fe2e129",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=hi_stoi[PAD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "21365877-68a0-4cb7-9464-68e59b0f9236",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7bb0f8cc-c70d-4c6c-9503-b3c947cffc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_dataset(enc_data, max_en_len, max_hi_len):\n",
    "    return[\n",
    "        (\n",
    "            pad(src, max_en_len, en_stoi[PAD]),\n",
    "            pad(tgt, max_hi_len, hi_stoi[PAD])\n",
    "        )\n",
    "        for src, tgt in enc_data\n",
    "    ]\n",
    "\n",
    "val_pad = pad_dataset(val_enc, max_en_len, max_hi_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2d5e0168-a357-4a48-869c-b59eabe1c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 60\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def train_epoch(model, data, optimizer, criterion, teacher_forcing_ratio):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    for i in range(0, len(data), BATCH_SIZE):\n",
    "        batch = data[i:i+BATCH_SIZE]\n",
    "        src = torch.tensor([x[0] for x in batch], dtype=torch.long).to(device)\n",
    "        tgt = torch.tensor([x[1] for x in batch], dtype=torch.long).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(src, tgt, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "\n",
    "        loss = criterion(\n",
    "            output[:, 1:].reshape(-1, output.size(-1)),\n",
    "            tgt[:, 1:].reshape(-1)\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / (len(data) // BATCH_SIZE + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ee3f1f8-59dc-49ec-a3db-7fb8b7654c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(data), BATCH_SIZE):\n",
    "            batch = data[i:i+BATCH_SIZE]\n",
    "            src = torch.tensor([x[0] for x in batch], dtype=torch.long).to(device)\n",
    "            tgt = torch.tensor([x[1] for x in batch], dtype=torch.long).to(device)\n",
    "\n",
    "            output = model(src, tgt, teacher_forcing_ratio=0.0)\n",
    "\n",
    "            loss = criterion(\n",
    "                output[:, 1:].reshape(-1, output.size(-1)),\n",
    "                tgt[:, 1:].reshape(-1)\n",
    "            )\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / (len(data) // BATCH_SIZE + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "de403e8f-0a9e-4145-a7f5-0562d25f0b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "Train Loss: 2.6590\n",
      "Val Loss: 2.3491\n",
      "----------------------------------------\n",
      "Epoch 2/60\n",
      "Train Loss: 1.6560\n",
      "Val Loss: 1.6673\n",
      "----------------------------------------\n",
      "Epoch 3/60\n",
      "Train Loss: 1.2342\n",
      "Val Loss: 1.4226\n",
      "----------------------------------------\n",
      "Epoch 4/60\n",
      "Train Loss: 1.0540\n",
      "Val Loss: 1.3295\n",
      "----------------------------------------\n",
      "Epoch 5/60\n",
      "Train Loss: 0.9535\n",
      "Val Loss: 1.3001\n",
      "----------------------------------------\n",
      "Epoch 6/60\n",
      "Train Loss: 0.8879\n",
      "Val Loss: 1.2758\n",
      "----------------------------------------\n",
      "Epoch 7/60\n",
      "Train Loss: 0.8354\n",
      "Val Loss: 1.2172\n",
      "----------------------------------------\n",
      "Epoch 8/60\n",
      "Train Loss: 0.7861\n",
      "Val Loss: 1.2049\n",
      "----------------------------------------\n",
      "Epoch 9/60\n",
      "Train Loss: 0.7486\n",
      "Val Loss: 1.1426\n",
      "----------------------------------------\n",
      "Epoch 10/60\n",
      "Train Loss: 0.7032\n",
      "Val Loss: 1.1637\n",
      "----------------------------------------\n",
      "Epoch 11/60\n",
      "Train Loss: 0.6842\n",
      "Val Loss: 1.1633\n",
      "----------------------------------------\n",
      "Epoch 12/60\n",
      "Train Loss: 0.6522\n",
      "Val Loss: 1.1494\n",
      "----------------------------------------\n",
      "Epoch 13/60\n",
      "Train Loss: 0.6285\n",
      "Val Loss: 1.1701\n",
      "----------------------------------------\n",
      "Epoch 14/60\n",
      "Train Loss: 0.6028\n",
      "Val Loss: 1.2002\n",
      "----------------------------------------\n",
      "Epoch 15/60\n",
      "Train Loss: 0.5671\n",
      "Val Loss: 1.1582\n",
      "----------------------------------------\n",
      "Epoch 16/60\n",
      "Train Loss: 0.5470\n",
      "Val Loss: 1.1740\n",
      "----------------------------------------\n",
      "Epoch 17/60\n",
      "Train Loss: 0.5255\n",
      "Val Loss: 1.2009\n",
      "----------------------------------------\n",
      "Epoch 18/60\n",
      "Train Loss: 0.5063\n",
      "Val Loss: 1.1861\n",
      "----------------------------------------\n",
      "Epoch 19/60\n",
      "Train Loss: 0.4804\n",
      "Val Loss: 1.2131\n",
      "----------------------------------------\n",
      "Epoch 20/60\n",
      "Train Loss: 0.4607\n",
      "Val Loss: 1.2157\n",
      "----------------------------------------\n",
      "Epoch 21/60\n",
      "Train Loss: 0.4395\n",
      "Val Loss: 1.2479\n",
      "----------------------------------------\n",
      "Epoch 22/60\n",
      "Train Loss: 0.4271\n",
      "Val Loss: 1.2236\n",
      "----------------------------------------\n",
      "Epoch 23/60\n",
      "Train Loss: 0.4158\n",
      "Val Loss: 1.2632\n",
      "----------------------------------------\n",
      "Epoch 24/60\n",
      "Train Loss: 0.3930\n",
      "Val Loss: 1.2816\n",
      "----------------------------------------\n",
      "Epoch 25/60\n",
      "Train Loss: 0.3790\n",
      "Val Loss: 1.2698\n",
      "----------------------------------------\n",
      "Epoch 26/60\n",
      "Train Loss: 0.3553\n",
      "Val Loss: 1.3595\n",
      "----------------------------------------\n",
      "Epoch 27/60\n",
      "Train Loss: 0.3414\n",
      "Val Loss: 1.3448\n",
      "----------------------------------------\n",
      "Epoch 28/60\n",
      "Train Loss: 0.3310\n",
      "Val Loss: 1.3826\n",
      "----------------------------------------\n",
      "Epoch 29/60\n",
      "Train Loss: 0.3160\n",
      "Val Loss: 1.4016\n",
      "----------------------------------------\n",
      "Epoch 30/60\n",
      "Train Loss: 0.3067\n",
      "Val Loss: 1.4038\n",
      "----------------------------------------\n",
      "Epoch 31/60\n",
      "Train Loss: 0.3000\n",
      "Val Loss: 1.4202\n",
      "----------------------------------------\n",
      "Epoch 32/60\n",
      "Train Loss: 0.2856\n",
      "Val Loss: 1.4279\n",
      "----------------------------------------\n",
      "Epoch 33/60\n",
      "Train Loss: 0.2777\n",
      "Val Loss: 1.4454\n",
      "----------------------------------------\n",
      "Epoch 34/60\n",
      "Train Loss: 0.2685\n",
      "Val Loss: 1.4873\n",
      "----------------------------------------\n",
      "Epoch 35/60\n",
      "Train Loss: 0.2554\n",
      "Val Loss: 1.5083\n",
      "----------------------------------------\n",
      "Epoch 36/60\n",
      "Train Loss: 0.2523\n",
      "Val Loss: 1.5288\n",
      "----------------------------------------\n",
      "Epoch 37/60\n",
      "Train Loss: 0.2458\n",
      "Val Loss: 1.5171\n",
      "----------------------------------------\n",
      "Epoch 38/60\n",
      "Train Loss: 0.2396\n",
      "Val Loss: 1.5519\n",
      "----------------------------------------\n",
      "Epoch 39/60\n",
      "Train Loss: 0.2443\n",
      "Val Loss: 1.5400\n",
      "----------------------------------------\n",
      "Epoch 40/60\n",
      "Train Loss: 0.2269\n",
      "Val Loss: 1.5609\n",
      "----------------------------------------\n",
      "Epoch 41/60\n",
      "Train Loss: 0.2211\n",
      "Val Loss: 1.5562\n",
      "----------------------------------------\n",
      "Epoch 42/60\n",
      "Train Loss: 0.2194\n",
      "Val Loss: 1.5971\n",
      "----------------------------------------\n",
      "Epoch 43/60\n",
      "Train Loss: 0.2084\n",
      "Val Loss: 1.6059\n",
      "----------------------------------------\n",
      "Epoch 44/60\n",
      "Train Loss: 0.2113\n",
      "Val Loss: 1.6274\n",
      "----------------------------------------\n",
      "Epoch 45/60\n",
      "Train Loss: 0.2054\n",
      "Val Loss: 1.6207\n",
      "----------------------------------------\n",
      "Epoch 46/60\n",
      "Train Loss: 0.2023\n",
      "Val Loss: 1.6382\n",
      "----------------------------------------\n",
      "Epoch 47/60\n",
      "Train Loss: 0.2069\n",
      "Val Loss: 1.6517\n",
      "----------------------------------------\n",
      "Epoch 48/60\n",
      "Train Loss: 0.1973\n",
      "Val Loss: 1.6491\n",
      "----------------------------------------\n",
      "Epoch 49/60\n",
      "Train Loss: 0.1909\n",
      "Val Loss: 1.6378\n",
      "----------------------------------------\n",
      "Epoch 50/60\n",
      "Train Loss: 0.1857\n",
      "Val Loss: 1.6971\n",
      "----------------------------------------\n",
      "Epoch 51/60\n",
      "Train Loss: 0.1894\n",
      "Val Loss: 1.6876\n",
      "----------------------------------------\n",
      "Epoch 52/60\n",
      "Train Loss: 0.1892\n",
      "Val Loss: 1.6867\n",
      "----------------------------------------\n",
      "Epoch 53/60\n",
      "Train Loss: 0.1864\n",
      "Val Loss: 1.7327\n",
      "----------------------------------------\n",
      "Epoch 54/60\n",
      "Train Loss: 0.1860\n",
      "Val Loss: 1.6777\n",
      "----------------------------------------\n",
      "Epoch 55/60\n",
      "Train Loss: 0.1782\n",
      "Val Loss: 1.7262\n",
      "----------------------------------------\n",
      "Epoch 56/60\n",
      "Train Loss: 0.1777\n",
      "Val Loss: 1.7026\n",
      "----------------------------------------\n",
      "Epoch 57/60\n",
      "Train Loss: 0.1755\n",
      "Val Loss: 1.7612\n",
      "----------------------------------------\n",
      "Epoch 58/60\n",
      "Train Loss: 0.1747\n",
      "Val Loss: 1.7542\n",
      "----------------------------------------\n",
      "Epoch 59/60\n",
      "Train Loss: 0.1700\n",
      "Val Loss: 1.7451\n",
      "----------------------------------------\n",
      "Epoch 60/60\n",
      "Train Loss: 0.1673\n",
      "Val Loss: 1.7667\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    # teacher_forcing_ratio = max(0.25, 0.6 * (0.97 ** epoch))\n",
    "    teacher_forcing_ratio = 0.5\n",
    "    train_loss = train_epoch(model, train_pad, optimizer, criterion, teacher_forcing_ratio)\n",
    "    val_loss = evaluate(model, val_pad, criterion)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d8cdc098-0ba3-4f06-b0c5-952f47ddcdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMON_FIXES = {\n",
    "    \"िि\": \"ि\",\n",
    "    \"ाा\": \"ा\",\n",
    "    \"ुु\": \"ु\",\n",
    "    \"ूू\": \"ू\",\n",
    "    \"ेे\": \"े\",\n",
    "    \"ोो\": \"ो\"\n",
    "}\n",
    "\n",
    "def normalize_hindi(text):\n",
    "    for b, g in COMMON_FIXES.items():\n",
    "        text = text.replace(b, g)\n",
    "    return text\n",
    "\n",
    "def postprocess_hindi(text):\n",
    "    # 1️⃣ Collapse duplicated matras (िि → ि, etc.)\n",
    "    for m in [\"ि\", \"ी\", \"ा\", \"ु\", \"ू\", \"े\", \"ो\"]:\n",
    "        text = text.replace(m + m, m)\n",
    "\n",
    "    # 2️⃣ Fix duplicated final consonant (ष्ट्ट → ष्ट)\n",
    "    if len(text) >= 2 and text[-1] == text[-2]:\n",
    "        text = text[:-1]\n",
    "\n",
    "    # 3️⃣ PROTECT common Hindi suffixes\n",
    "    protected_suffixes = (\n",
    "        \"स्थान\", \"पुर\", \"नगर\", \"गंज\", \"गढ़\", \"पुरम\"\n",
    "    )\n",
    "    for suf in protected_suffixes:\n",
    "        if text.endswith(suf):\n",
    "            return text   # DO NOTHING further\n",
    "\n",
    "    # 4️⃣ Trim hallucinated trailing junk ONLY if long\n",
    "    if len(text) >= 7:\n",
    "        # remove trailing vowels like \"जो\", \"यी\", \"ऊ\"\n",
    "        if text[-1] in {\"ो\", \"ू\", \"ी\"}:\n",
    "            text = text[:-1]\n",
    "\n",
    "        # remove trailing filler consonants\n",
    "        if text[-1] in {\"य\", \"र\"}:\n",
    "            text = text[:-1]\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a415456c-70c1-40ae-b674-ce20c9199257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_transliterate(model, word, beam_width=3, max_len=40):\n",
    "    model.eval()\n",
    "\n",
    "    src = torch.tensor([encode(word.lower(), en_stoi)], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, h, c = model.encoder(src)\n",
    "\n",
    "    beams = [([hi_stoi[SOS]], 0.0, h, c)]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        new_beams = []\n",
    "        for seq, score, h, c in beams:\n",
    "            if seq[-1] == hi_stoi[EOS]:\n",
    "                new_beams.append((seq, score, h, c))\n",
    "                continue\n",
    "\n",
    "            input_tok = torch.tensor([[seq[-1]]], device=device)\n",
    "            with torch.no_grad():\n",
    "                logits, h_new, c_new = model.decoder(input_tok, h, c, encoder_outputs)\n",
    "\n",
    "            log_probs = torch.log_softmax(logits[0], dim=-1)\n",
    "            topk = torch.topk(log_probs, beam_width)\n",
    "\n",
    "            for idx, val in zip(topk.indices, topk.values):\n",
    "                new_beams.append(\n",
    "                    (seq + [idx.item()], score + val.item(), h_new, c_new)\n",
    "                )\n",
    "\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "        if all(seq[-1] == hi_stoi[EOS] for seq, _, _, _ in beams):\n",
    "            break\n",
    "\n",
    "    best = beams[0][0]\n",
    "    out = \"\".join(\n",
    "        hi_itos[i] for i in best\n",
    "        if i not in {hi_stoi[SOS], hi_stoi[EOS], hi_stoi[PAD]}\n",
    "    )\n",
    "    out = normalize_hindi(out)\n",
    "    out = postprocess_hindi(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "004af1e0-5c3a-4702-a4e6-0771f695e787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : Delhi\n",
      "Beam:   दिल्ली\n",
      "------------------------------\n",
      "Input : Kolkata\n",
      "Beam:   कोलकत्त\n",
      "------------------------------\n",
      "Input : Bangalore\n",
      "Beam:   बान्गोल\n",
      "------------------------------\n",
      "Input : Rajasthan\n",
      "Beam:   राजस्थान\n",
      "------------------------------\n",
      "Input : Chandrakala\n",
      "Beam:   चन्द्रकाल\n",
      "------------------------------\n",
      "Input : Vishnupuram\n",
      "Beam:   विष्णुपुरम\n",
      "------------------------------\n",
      "Input : Maharashta\n",
      "Beam:   माराष्ट्\n",
      "------------------------------\n",
      "Input : Kaveri\n",
      "Beam:   कवेरि\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "tests = [\n",
    "    \"Delhi\",\n",
    "    \"Kolkata\",\n",
    "    \"Bangalore\",\n",
    "    \"Rajasthan\",\n",
    "    \"Chandrakala\",\n",
    "    \"Vishnupuram\",\n",
    "    \"Maharashta\",\n",
    "    \"Kaveri\"\n",
    "]\n",
    "\n",
    "for w in tests:\n",
    "    print(\"Input :\", w)\n",
    "    print(\"Beam:  \", beam_transliterate(model, w))\n",
    "    # print(\"Greedy: \", greedy_transliterate(model,w))\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "38d5afb6-8451-4bc8-bab7-ee37e489b136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_accuracy(pred, gold):\n",
    "    correct = 0\n",
    "    total = max(len(pred), len(gold))\n",
    "\n",
    "    for p, g in zip(pred, gold):\n",
    "        if p == g:\n",
    "            correct += 1\n",
    "\n",
    "    return correct / total if total > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "456e877f-5376-40e8-b58d-ac038f335cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_char_accuracy(model, data):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "\n",
    "    for en, hi in data:\n",
    "        pred = beam_transliterate(model, en)\n",
    "        scores.append(char_accuracy(pred, hi))\n",
    "\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aac5cfdd-d366-4de8-bb7f-55ad38bf2ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Char Accuracy:  46.06 %\n",
      "Test Char Accuracy:  46.2 %\n"
     ]
    }
   ],
   "source": [
    "val_acc = evaluate_char_accuracy(model, val)\n",
    "test_acc = evaluate_char_accuracy(model, test)\n",
    "\n",
    "print(\"Validation Char Accuracy: \", round(val_acc * 100, 2), \"%\")\n",
    "print(\"Test Char Accuracy: \", round(test_acc * 100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "256ec9eb-0a58-49de-80ff-5c11dc4988de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as seq2seq_model.pt\n"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"en_stoi\": en_stoi,\n",
    "    \"hi_stoi\": hi_stoi,\n",
    "    \"hi_itos\": hi_itos,\n",
    "    \"PAD\": PAD,\n",
    "    \"SOS\": SOS,\n",
    "    \"EOS\": EOS,\n",
    "    \"EMBED_DIM\": EMBED_DIM,\n",
    "    \"HIDDEN_DIM\": HIDDEN_DIM\n",
    "}, \"seq2seq_model.pt\")\n",
    "\n",
    "print(\"Model saved as seq2seq_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8708183b-d403-4dac-be3a-23a1f04e521a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (translit)",
   "language": "python",
   "name": "translit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
